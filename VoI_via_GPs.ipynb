{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform VoI calculation using GP surrogate over join action-uncertainty space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import qmc\n",
    "import sklearn.gaussian_process as gp\n",
    "import scipy.optimize as op\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri as tri\n",
    "\n",
    "import inspect\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base parameters.\n",
    "dataset_dir = os.path.join('data','A37_analysis_test') # dataset directory\n",
    "opex_factor = 10\n",
    "pricing_dict = {'carbon':5e-1,'battery':1e3,'solar':2e3}\n",
    "with open(os.path.join(dataset_dir,'metadata_ext.json'),'r') as json_file:\n",
    "        annex_defaults = json.load(json_file)\n",
    "base_kwargs = {\n",
    "    'output_dir_path': dataset_dir,\n",
    "    'building_names': ['UCam_Building_%s'%id for id in ids],\n",
    "    'battery_energy_capacities': None,\n",
    "    'battery_power_capacities': [annex_defaults[\"building_attributes\"][\"battery_power_capacities (kW)\"][str(id)] for id in ids],\n",
    "    'battery_efficiencies': None,\n",
    "    'pv_power_capacities': None,\n",
    "    'load_data_paths': ['UCam_Building_%s.csv'%id for id in ids],\n",
    "    'weather_data_path': 'weather.csv',\n",
    "    'carbon_intensity_data_path': 'carbon_intensity.csv',\n",
    "    'pricing_data_path': 'pricing.csv',\n",
    "    'schema_name': 'schema_temp'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiprocessing helper functions.\n",
    "\n",
    "n_processes = min(25,os.cpu_count()//2) # for some reason more than this is an issue ...? Maybe a memory issue\n",
    "\n",
    "def multi_proc_constr_and_eval_system(args_list):\n",
    "\n",
    "    from sys_eval import construct_and_evaluate_system\n",
    "\n",
    "    return construct_and_evaluate_system(*args_list)\n",
    "\n",
    "def parallel_task(func, iterable, n_procs):\n",
    "    # Adapted from solution to https://stackoverflow.com/questions/47313732/jupyter-notebook-never-finishes-processing-using-multiprocessing-python-3/47374811\n",
    "    # NOTE: this function must be in the notebook\n",
    "    # NOTE: in this workaround, the function passed to `parallel_task`\n",
    "    # must do all of the importing it needs, and unwrap the arguments\n",
    "    # (only a single argument can be passed)\n",
    "\n",
    "    temp_path = f'./tmp_func.py'\n",
    "    with open(temp_path, 'w') as file:\n",
    "        file.write(inspect.getsource(func).replace(func.__name__, \"task\"))\n",
    "\n",
    "    from tmp_func import task\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        pool = Pool(processes=n_procs)\n",
    "        res = list(tqdm(pool.imap(task, iterable), total=len(iterable)))\n",
    "        pool.close()\n",
    "        os.remove(temp_path)\n",
    "        return res\n",
    "    else:\n",
    "        raise \"Not in Jupyter Notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Train GP Surrogate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: probably sensible to conduct an initial rough space search to identify sensible bounds as we expect the objective (cost) to be behaviour reasonably nicely, at least far from the region of the optimum.\n",
    "The smaller the bounded region the better the surrogate will be and the easier the global optimisation will be - however as these bounds are arbitrary, if we encounter boundary solutions, the bounds need to be extended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remeber to note Dom's alternative suggestion for learning a surrogate, and it's motivation from the smoothness of his posterior solution results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sampling of action-uncertainty space\n",
    "seed = 42\n",
    "n_samples = 5000 # increase for final results?\n",
    "\n",
    "# Space filling sampling of action sub-space using Latin Hypercube\n",
    "lower_bounds = np.array([*[1.25e3]*len(ids),*[9e2]*len(ids)])\n",
    "upper_bounds = np.array([*[1.5e3]*len(ids),*[1.15e3]*len(ids)])\n",
    "\n",
    "action_sampler = qmc.LatinHypercube(d=len(ids)*2)\n",
    "action_samples = action_sampler.random(n=n_samples)\n",
    "action_samples = qmc.scale(action_samples, lower_bounds, upper_bounds)\n",
    "\n",
    "# Sample from prior distributions of uncertain parameters\n",
    "mu = 0.85\n",
    "sigma = 0.1\n",
    "eta_samples = np.random.normal(loc=mu,scale=sigma,size=(n_samples,len(ids)))\n",
    "eta_samples = np.clip(eta_samples,0,1)\n",
    "\n",
    "# Combine samples from two sub-spaces\n",
    "space_samples = np.hstack([action_samples,eta_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate system cost over samples. (Old serial implementation)\n",
    "# costs = []\n",
    "# for n in tqdm(range(n_samples)):\n",
    "#     cost = construct_and_evaluate_system(action_samples[n][:len(ids)],action_samples[n][len(ids):],eta_samples[n],base_kwargs,pricing_dict,opex_factor)\n",
    "#     costs.append(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate system cost over samples. (Parallelised)\n",
    "mproc_args_list = [[action_samples[n][:len(ids)],action_samples[n][len(ids):],eta_samples[n],base_kwargs,pricing_dict,opex_factor,n] for n in range(n_samples)]\n",
    "space_sample_costs = parallel_task(multi_proc_constr_and_eval_system, mproc_args_list, n_procs=n_processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gaussian Process surrogate.\n",
    "\n",
    "length_scale_init_guess = [50,40,0.05]\n",
    "length_scale_bounds = [(10,200),(10,200),(0.005,0.25)]\n",
    "kernel = 1 * gp.kernels.RBF(length_scale=np.array(length_scale_init_guess),length_scale_bounds=length_scale_bounds) # + gp.kernels.WhiteKernel(noise_level=1)\n",
    "gp_surrogate = gp.GaussianProcessRegressor(kernel=kernel,normalize_y=True, n_restarts_optimizer=5)\n",
    "start = time.time()\n",
    "gp_surrogate.fit(space_samples, space_sample_costs)\n",
    "end = time.time()\n",
    "gp_surrogate.kernel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"GP surrogate training time: {round(end-start,1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE!!! Training the GP properly seems to be a bit of an issue - the length scales that comes out of the optimiser are sometimes completely off - having a good initialisation to the optimisation seems important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_prediction, std_prediction = gp_surrogate.predict(space_samples, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual surrogate model.\n",
    "\n",
    "# Create set of points in action sub-space to interpolate to\n",
    "n_predict_points = 2500\n",
    "# action_predict_sampler = qmc.LatinHypercube(d=len(ids)*2)\n",
    "# action_predict_samples = action_predict_sampler.random(n=n_predict_points)\n",
    "# action_predict_samples = qmc.scale(action_predict_samples, lower_bounds, upper_bounds)\n",
    "xs = np.linspace(lower_bounds[0],upper_bounds[0],num=int(np.sqrt(n_predict_points)))\n",
    "ys = np.linspace(lower_bounds[1],upper_bounds[1],num=int(np.sqrt(n_predict_points)))\n",
    "xx,yy = np.meshgrid(xs,ys)\n",
    "action_predict_samples = np.array([(x,y) for x,y in zip(xx.flatten(),yy.flatten())])\n",
    "\n",
    "# Compute mean cost predicted by surrogate model.\n",
    "cost_predictions = []\n",
    "\n",
    "n_MC_samples = 5000\n",
    "for sample in tqdm(action_predict_samples):\n",
    "    eta_samples = np.random.normal(loc=mu,scale=sigma,size=(n_MC_samples,len(ids)))\n",
    "    eta_samples = np.clip(eta_samples,0,1)\n",
    "    predict_points = np.hstack([np.tile(sample,(n_MC_samples,1)),eta_samples])\n",
    "    mean_prediction = gp_surrogate.predict(predict_points)\n",
    "    cost_predictions.append(np.mean(mean_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot estatimate mean cost prediction from surrogate.\n",
    "assert len(ids) == 1\n",
    "\n",
    "n_levels = 20\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "#triang = tri.Triangulation(action_predict_samples[:,0],action_predict_samples[:,1])\n",
    "#tcf = ax.tricontourf(triang, cost_predictions, levels=n_levels)\n",
    "#ax.tricontour(triang, cost_predictions, levels=n_levels, colors='k')\n",
    "#ax.plot(action_predict_samples[:,0],action_predict_samples[:,1],'ko',alpha=0.25)\n",
    "tcf = ax.contourf(xx,yy,np.array(cost_predictions).reshape(xx.shape),levels=n_levels)\n",
    "ax.contour(xx,yy,np.array(cost_predictions).reshape(xx.shape),levels=n_levels,colors='k')\n",
    "#ax.plot(xx.flatten(),yy.flatten(),'ko',alpha=0.25)\n",
    "fig.colorbar(tcf)\n",
    "ax.set_title(\"Surrogate prediction of mean system cost\")\n",
    "ax.set_xlabel(\"Battery energy capacity (kWh)\")\n",
    "ax.set_ylabel(\"Solar power capacity (kWp)\")\n",
    "ax.set_xlim(lower_bounds[0],upper_bounds[0])\n",
    "ax.set_ylim(lower_bounds[1],upper_bounds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lumpier the objective near the true optimum, the worse job the surrogate will do in accurately representing the optimisation. However, this is a feature of the difficultness of the problem, as the global optimisation of the direct MC estimate will also struggle (i.e. require more fevals). The way we deal with this is by adding more sample points to the GP.\n",
    "\n",
    "Potentially a smarter strategy is to use a mutil-fidelity modelling approach, and refine the bounds on the sampled points iteratively to the subdomain in which the optimal values lie (for likely samples of the uncertain variables), as only this reigon contributes to the computed VoI value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cost predicted by surrogate model for specific eta.\n",
    "eta_samples = np.random.normal(loc=mu,scale=sigma,size=(len(ids)))\n",
    "eta_samples = np.clip(eta_samples,0,1)\n",
    "eta_samples = [0.62,0.82,1]\n",
    "\n",
    "for eta in eta_samples:\n",
    "    se_cost_predictions = []\n",
    "    for sample in tqdm(action_predict_samples):\n",
    "        predict_points = np.hstack([sample,eta])\n",
    "        mean_prediction = gp_surrogate.predict([predict_points])\n",
    "        se_cost_predictions.append(np.mean(mean_prediction))\n",
    "\n",
    "    # Plot estatimate mean cost prediction from surrogate.\n",
    "    assert len(ids) == 1\n",
    "\n",
    "    n_levels = 20\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    #triang = tri.Triangulation(action_predict_samples[:,0],action_predict_samples[:,1])\n",
    "    #tcf = ax.tricontourf(triang,se_cost_predictions,levels=n_levels)\n",
    "    #ax.tricontour(triang, se_cost_predictions,levels=n_levels, colors='k')\n",
    "    tcf = ax.contourf(xx,yy,np.array(se_cost_predictions).reshape(xx.shape),levels=n_levels)\n",
    "    ax.contour(xx,yy,np.array(se_cost_predictions).reshape(xx.shape),levels=n_levels,colors='k')\n",
    "    fig.colorbar(tcf)\n",
    "    ax.set_title(f\"Surrogate prediction of system cost for eta=%s\"%round(eta,4))\n",
    "    ax.set_xlim(lower_bounds[0],upper_bounds[0])\n",
    "    ax.set_ylim(lower_bounds[1],upper_bounds[1])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate convergence of MC estimate using surrogate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_point = [1377.9, 1023.8]\n",
    "eta_samples = np.random.normal(loc=mu,scale=sigma,size=(n_MC_samples,len(ids)))\n",
    "eta_samples = np.clip(eta_samples,0,1)\n",
    "predict_points = np.hstack([np.tile(sample_point,(n_MC_samples,1)),eta_samples])\n",
    "mean_prediction = gp_surrogate.predict(predict_points)\n",
    "\n",
    "MC_means = [np.mean(mean_prediction[:i]) for i in range(1,n_MC_samples+1)]\n",
    "MC_stnd_errors = [np.std(mean_prediction[:i])/np.sqrt(i) for i in range(2,n_MC_samples+1)]\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.set_xlim(1,n_MC_samples)\n",
    "ax.set_xlabel(\"Number of samples\")\n",
    "ax.grid(True,'major',alpha=0.5,linestyle='--')\n",
    "\n",
    "ln1 = ax.plot(range(1,n_MC_samples+1),np.array(MC_means)/1e6,'-k',label=\"Mean cost\")\n",
    "ax.set_ylabel('Mean cost estimate (£m)')\n",
    "#ax.set_ylim(0)\n",
    "\n",
    "# min_ax = ax.twinx()\n",
    "# min_ax.plot(range(2,n_draws+1),(np.array(MC_stnd_errors)/MC_means[-1])*100,'k--')\n",
    "# min_ax.set_ylabel('Estimate standard error (% of final mean)')\n",
    "# min_ax.set_ylim(0)\n",
    "\n",
    "min_ax_abs = ax.twinx()\n",
    "ln2 = min_ax_abs.plot(range(2,n_MC_samples+1),np.array(MC_stnd_errors)/1e3,'k:',label=\"Standard error\")\n",
    "min_ax_abs.set_ylabel('Estimate standard error (£k)')\n",
    "min_ax_abs.set_ylim(0)\n",
    "\n",
    "lns = ln1+ln2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax.legend(lns,labs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Solve Prior Problem using Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fn of mean system cost predicted by surrogate.\n",
    "def mean_cost_surrogate_estimate(x, surrogate, n_MC_samples, eta_mu, eta_sigma):\n",
    "\n",
    "    assert len(x) % 2 == 0, \"Design variable argument must have even length.\"\n",
    "\n",
    "    # Make draw from distribution of uncertainties - specified in args (mu, sigma)\n",
    "    # ========================================================================\n",
    "    eta_samples = np.random.normal(loc=eta_mu,scale=eta_sigma,size=(n_MC_samples,len(ids)))\n",
    "    eta_samples = np.clip(eta_samples,0,1)\n",
    "\n",
    "    # Predict costs for each efficiency sample at target point using surrogate\n",
    "    # ========================================================================\n",
    "    predict_points = np.hstack([np.tile(x,(n_MC_samples,1)),eta_samples])\n",
    "    mean_prediction = surrogate.predict(predict_points)\n",
    "\n",
    "    return np.mean(mean_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = op.Bounds(lb=lower_bounds,ub=upper_bounds)\n",
    "n_surrogate_MC_samples = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "prior_soln = op.differential_evolution(mean_cost_surrogate_estimate, bounds, args=(gp_surrogate,n_surrogate_MC_samples,mu,sigma), seed=seed)\n",
    "end = time.time()\n",
    "print(prior_soln.x, prior_soln.fun, prior_soln.nfev, prior_soln.message)\n",
    "prior_design = prior_soln.x\n",
    "prior_cost = prior_soln.fun\n",
    "print(f\"Prior solution: {prior_design}\")\n",
    "print(f\"Predicted prior solution cost: {round(prior_cost,2)}\")\n",
    "print(f\"Prior problem solve time: {round(end-start,1)}\")\n",
    "print(prior_soln.nfev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test quality of surrogate prior optimal solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_MC_samples = 500\n",
    "eta_samples = np.random.normal(loc=mu,scale=sigma,size=(n_MC_samples,len(ids)))\n",
    "eta_samples = np.clip(eta_samples,0,1)\n",
    "\n",
    "# sample_costs = []\n",
    "# for eta_sample in tqdm(eta_samples):\n",
    "#     sample_cost = construct_and_evaluate_system(prior_design[:len(ids)],prior_design[len(ids):],eta_sample,base_kwargs,pricing_dict,opex_factor)\n",
    "#     sample_costs.append(sample_cost)\n",
    "mproc_args_list = [[prior_design[:len(ids)],prior_design[len(ids):],eta_samples[n],base_kwargs,pricing_dict,opex_factor,n] for n in range(n_MC_samples)]\n",
    "sample_costs = parallel_task(multi_proc_constr_and_eval_system, mproc_args_list, n_procs=n_processes)\n",
    "\n",
    "true_prior_soln_cost_estimate = np.mean(sample_costs)\n",
    "\n",
    "print(f\"True prior solution cost: {round(true_prior_soln_cost_estimate,2)}\")\n",
    "print(f\"Surrogate prior optimal solution fn error: {round(np.abs(true_prior_soln_cost_estimate-prior_cost),2)},\\\n",
    "    {round((np.abs(true_prior_soln_cost_estimate-prior_cost)/true_prior_soln_cost_estimate)*100,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare absolute error to MC estimate standard error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.std(sample_costs)/np.sqrt(len(sample_costs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Solve Pre-Posterior Problem using Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from prior distribution of uncertainties.\n",
    "n_prior_samples = 200\n",
    "prior_eta_samples = np.random.normal(loc=mu,scale=sigma,size=(n_prior_samples,len(ids)))\n",
    "prior_eta_samples = np.clip(prior_eta_samples,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_sigma = 0.01 # define measurment uncertainty\n",
    "\n",
    "posterior_optimal_results = []\n",
    "prior_soln_performances = []\n",
    "\n",
    "# Solve posterior problem for each eta sample.\n",
    "for eta_sample in tqdm(prior_eta_samples):\n",
    "    posterior_soln = op.differential_evolution(mean_cost_surrogate_estimate, bounds, args=(gp_surrogate,n_surrogate_MC_samples,eta_sample,posterior_sigma), seed=seed)\n",
    "    posterior_optimal_results.append(posterior_soln)\n",
    "    #print(eta_sample, posterior_soln.x, posterior_soln.fun, posterior_soln.message)\n",
    "\n",
    "    # check surrogate predicted performance (cost) of prior solution for posterior given sampled eta value\n",
    "    prior_soln_eta_cost = mean_cost_surrogate_estimate(prior_design,gp_surrogate,n_surrogate_MC_samples,eta_sample,posterior_sigma)\n",
    "    prior_soln_performances.append(prior_soln_eta_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preposterior_costs = [res.fun for res in posterior_optimal_results]\n",
    "preposterior_cost = np.mean(preposterior_costs)\n",
    "print(\"Pre-posterior cost: %s\"%preposterior_cost)\n",
    "print(\"Prior cost check: \",np.mean(prior_soln_performances),prior_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The two methods of calculating prior solution cost return quite different values - which one is better to use? Potentially the one computed using the nested expectations as it uses the same posterior cost curve estimates (from the surrogate) that is used for the posterior optimisation. But on the other hand, the first method is one used to determine the prior solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence of pre-posterior cost MC estimate.\n",
    "\n",
    "MC_means = [np.mean(preposterior_costs[:i]) for i in range(1,n_prior_samples+1)]\n",
    "MC_stnd_errors = [np.std(preposterior_costs[:i])/np.sqrt(i) for i in range(2,n_prior_samples+1)]\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.set_xlim(1,n_prior_samples)\n",
    "ax.set_xlabel(\"Number of samples\")\n",
    "ax.grid(True,'major',alpha=0.5,linestyle='--')\n",
    "\n",
    "ln1 = ax.plot(range(1,n_prior_samples+1),np.array(MC_means)/1e6,'-k',label='Mean cost')\n",
    "ax.set_ylabel('Pre-posterior cost estimate (£m)')\n",
    "#ax.set_ylim(0)\n",
    "\n",
    "# min_ax = ax.twinx()\n",
    "# min_ax.plot(range(2,n_draws+1),(np.array(MC_stnd_errors)/MC_means[-1])*100,'k--')\n",
    "# min_ax.set_ylabel('Estimate standard error (% of final mean)')\n",
    "# min_ax.set_ylim(0)\n",
    "\n",
    "min_ax_abs = ax.twinx()\n",
    "ln2 = min_ax_abs.plot(range(2,n_prior_samples+1),np.array(MC_stnd_errors)/1e3,'k:',label='Standard error')\n",
    "min_ax_abs.set_ylabel('Estimate standard error (£k)')\n",
    "min_ax_abs.set_ylim(0)\n",
    "\n",
    "lns = ln1+ln2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax.legend(lns,labs)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot map of posterior optimal solutions.\n",
    "assert len(ids) == 1\n",
    "\n",
    "posterior_optimal_solutions = np.array([res.x for res in posterior_optimal_results])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cm = plt.cm.get_cmap()\n",
    "sc = plt.scatter(posterior_optimal_solutions[:,0], posterior_optimal_solutions[:,1], c=preposterior_costs,cmap=cm)\n",
    "#plt.scatter(prior_design[0],prior_design[1],c='k')\n",
    "ax.set_title(\"Posterior optimal solutions from surrogate (by cost)\")\n",
    "ax.set_xlabel(\"Battery energy capacity (kWh)\")\n",
    "ax.set_ylabel(\"Solar power capacity (kWp)\")\n",
    "ax.set_xlim(lower_bounds[0],upper_bounds[0])\n",
    "ax.set_ylim(lower_bounds[1],upper_bounds[1])\n",
    "plt.colorbar(sc)\n",
    "#plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cm = plt.cm.get_cmap('RdYlBu')\n",
    "sc = plt.scatter(posterior_optimal_solutions[:,0], posterior_optimal_solutions[:,1], c=prior_eta_samples,cmap=cm)\n",
    "#plt.scatter(prior_design[0],prior_design[1],c='k')\n",
    "ax.set_title(\"Posterior optimal solutions from surrogate (by eta)\")\n",
    "ax.set_xlabel(\"Battery energy capacity (kWh)\")\n",
    "ax.set_ylabel(\"Solar power capacity (kWp)\")\n",
    "ax.set_xlim(lower_bounds[0],upper_bounds[0])\n",
    "ax.set_ylim(lower_bounds[1],upper_bounds[1])\n",
    "plt.colorbar(sc)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These would be really great plots to use to iterate the state-space bounds!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a highly repeated solution - though this time it's not the same are the prior solution\n",
    "print(np.sum([1 if np.linalg.norm(item-[1373.46,1002.84]) < 1 else 0 for item in posterior_optimal_solutions]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test quality of surrogate posterior optimal solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - this calculation is extremely hefty! However, we can use a random sub-sample of solutions to estimate mean quality (note solutions are already randomly ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_MC_samples = 100\n",
    "n_solns_used = 100\n",
    "\n",
    "true_posterior_soln_cost_estimates = []\n",
    "\n",
    "# Compute direct MC estimate of optimal solution cost for each eta sample.\n",
    "for i, (posterior_result,eta_sample) in tqdm(enumerate(zip(posterior_optimal_results,prior_eta_samples)),total=n_solns_used):\n",
    "    if i < n_solns_used:\n",
    "        posterior_eta_samples = np.random.normal(loc=eta_sample,scale=posterior_sigma,size=(n_MC_samples,len(ids)))\n",
    "        posterior_eta_samples = np.clip(posterior_eta_samples,0,1)\n",
    "        mproc_args_list = [[posterior_result.x[:len(ids)],posterior_result.x[len(ids):],posterior_eta_samples[n],base_kwargs,pricing_dict,opex_factor,n] for n in range(n_MC_samples)]\n",
    "        posterior_sample_costs = parallel_task(multi_proc_constr_and_eval_system, mproc_args_list, n_procs=n_processes)\n",
    "        true_posterior_soln_cost_estimates.append(np.mean(posterior_sample_costs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted posterior solution costs: \",preposterior_costs[:n_solns_used])\n",
    "print(\"Estimates of true posterior solution costs: \",true_posterior_soln_cost_estimates)\n",
    "mean_posterior_soln_fn_error = np.mean(np.abs(np.array(true_posterior_soln_cost_estimates)-np.array(preposterior_costs[:n_solns_used])))\n",
    "mean_posterior_soln_fn_error_perc = np.mean((np.abs(np.array(true_posterior_soln_cost_estimates)-np.array(preposterior_costs[:n_solns_used]))/np.array(true_posterior_soln_cost_estimates))*100)\n",
    "print(f\"Surrogate mean posterior optimal solution fn error: {round(mean_posterior_soln_fn_error,2)}, {round(mean_posterior_soln_fn_error_perc,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph posterior solution error results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Report VOI Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the standard error on the pre-posterior cost MC estimate as an error for result reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"VOI: %s\"%round(prior_cost-preposterior_cost,2))\n",
    "print(\"plus-minus: %s\"%round(np.std(preposterior_costs)/np.sqrt(n_prior_samples),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prior_cost,preposterior_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prior cost check: \",np.mean(prior_soln_performances),prior_cost)\n",
    "print(\"Regret based VoI: \",round((np.mean(prior_soln_performances)-preposterior_cost),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot impact of efficiency on total system cost.\n",
    "assert len(ids) == 1\n",
    "\n",
    "trend = np.polyfit(prior_eta_samples.flatten(),preposterior_costs,deg=1)\n",
    "\n",
    "plt.scatter(prior_eta_samples,np.array(preposterior_costs)/1e6,c='k',label='Posterior')\n",
    "plt.plot(prior_eta_samples,(prior_eta_samples*trend[0] + trend[1])/1e6,'k-',alpha=0.5)\n",
    "plt.scatter(prior_eta_samples,np.array(prior_soln_performances)/1e6,label='Prior')\n",
    "plt.xlabel(\"Battery efficiency\")\n",
    "plt.ylabel(\"Total system cost (£m)\")\n",
    "plt.legend(title=\"Solution point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Battery efficiency has a significant impact on the system cost, but knowing it a priori does not impact the optimal system design solution significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_soln_subopts = np.array(prior_soln_performances)-np.array(preposterior_costs)\n",
    "plt.hlines(np.mean(prior_soln_subopts)/1e3,min(prior_eta_samples),max(prior_eta_samples),'k',ls='--')\n",
    "plt.scatter(prior_eta_samples,(prior_soln_subopts)/1e3)\n",
    "plt.hlines(0,min(prior_eta_samples),1,colors='k')\n",
    "plt.xlabel(\"Battery efficiency\")\n",
    "plt.ylabel(\"Prior solution cost sub-optimality (£k)\")\n",
    "plt.ylim(-2,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, some values have been clipped (LHS, very high, corresponding to posterior solutions on edge of action region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instances in the which the prior solution is better than the posterior solution (negative sub-optimalities) are artefacts of the global optimisation and MC estimate errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VOI-CAS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
